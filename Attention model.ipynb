{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Language models with attention\n",
    "\n",
    "Step by step guide to implementing an Attention model from first principles in Tensorflow based on the notes from the [Oxford Deep NLP course](https://github.com/oxford-cs-deepnlp-2017/lectures/blob/master/Lecture%208%20-%20Conditional%20Language%20Modeling%20with%20Attention.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate test data\n",
    "\n",
    "To start with we need to create some input and output data, taking into account the fact that this data might have different lengths. Let us randomly generate some inputs and outputs. These will represent sentences which have already been processed so that words are represented by integer word ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([58, 73, 16, 73, 54, 69]), array([91, 18, 85, 84, 16,  5]), array([41, 35,  7, 13, 98, 47, 58]), array([38,  5, 72, 71, 41]), array([38, 37, 83, 51, 98, 26, 10, 66, 92, 13]), array([27, 66, 92, 95, 13, 49]), array([67, 41, 47, 63, 99, 89, 24]), array([18, 48, 79, 71, 27, 16, 87]), array([100,  39,  21,  50,  25]), array([ 6, 84, 99, 77,  4, 29, 94, 89, 67, 25]), array([51, 75, 55,  8, 25, 64]), array([93, 58, 20, 81, 25]), array([31, 49, 86, 76, 84, 64]), array([89, 57, 39, 43, 64, 89, 48]), array([81, 70, 26, 89, 81, 19, 60, 97,  4]), array([69, 21, 80, 83,  2, 61, 63, 84]), array([17, 59, 73, 65, 47, 79, 80]), array([92, 91, 84, 44, 87]), array([ 34,  54,  69,  17,  27,  52,  33, 100,  41]), array([11, 27, 46, 32, 56, 99]), array([39, 41, 75, 52, 58, 26]), array([73,  3,  7, 83, 24, 13, 41, 15,  2]), array([  2, 100,  67,  52,  58]), array([94, 96, 21,  6,  7, 29,  4, 75, 34, 16]), array([76, 80, 39, 56, 37, 90, 45])]\n",
      "[array([92,  6, 64, 44, 49]), array([10, 74, 84, 31, 40,  2, 31, 61]), array([84, 33, 20, 37, 84, 77]), array([53, 82, 32, 84, 75, 43, 69, 67, 84, 55]), array([71, 36, 16, 86, 35]), array([41, 43,  6, 46, 88, 83, 54,  4]), array([ 3, 76, 51, 57, 88, 75, 34, 83, 22,  8]), array([  8,   4,  57,  76,  29, 100,  18,  95]), array([76, 46, 79, 44, 49, 94, 21, 40]), array([75, 59, 59, 64, 75, 42, 46, 49, 35, 10]), array([92, 30, 22, 70, 42, 15, 18]), array([87, 95, 55, 23, 26, 72, 73, 89,  9, 59]), array([22, 79, 69,  9, 88, 56]), array([22, 82, 30, 98, 63, 25]), array([61, 26, 44, 77, 52, 93, 24, 32, 95]), array([53, 75, 75, 58, 69]), array([31, 86, 75, 30, 74, 44, 15, 51, 76]), array([92, 39, 66, 85, 95, 64, 39, 10, 14, 20]), array([76, 48,  5, 92, 53, 45, 81, 68, 53, 97]), array([74, 82, 90,  8, 51,  4]), array([34, 70, 33, 51, 32, 61, 58, 21, 84]), array([56, 14,  9, 78, 51, 73]), array([94, 87, 91, 50, 65, 14, 23, 96]), array([30, 68, 61, 23, 99, 78, 41]), array([97, 69, 75, 79, 76, 15, 37, 50])]\n"
     ]
    }
   ],
   "source": [
    "# generate arrays of random numbers where the array length is a random number between 5 and 11\n",
    "source_inputs = [np.random.randint(2,101,n) for n in  np.random.randint(5,11,25)]\n",
    "target_inputs = [np.random.randint(2,101,n) for n in  np.random.randint(5,11,25)]\n",
    "\n",
    "print(source_inputs)\n",
    "print(target_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For use in the RNN, let us calculate sequence lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([7, 8, 6, 6, 6, 6, 8, 5, 5, 9, 8, 10, 8, 8, 10, 6, 9, 9, 8, 5, 5, 9, 5, 8, 7], [8, 5, 6, 6, 7, 5, 10, 7, 5, 5, 8, 10, 6, 7, 9, 9, 10, 8, 7, 6, 6, 8, 5, 9, 8])\n"
     ]
    }
   ],
   "source": [
    "# check lengths of inputs and targets\n",
    "source_lens = list(map(len,source_inputs))\n",
    "target_lens = list(map(len,target_inputs))\n",
    "\n",
    "print(source_lens, target_lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad the input and output arrays so they are of length 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 29.,  77.,  41.,  18.,  79.,   5.,  46.,   0.,   0.,   0.],\n",
       "       [ 13.,  45.,  82.,  93.,   9.,  95.,  75.,  42.,   0.,   0.],\n",
       "       [ 37.,  92.,  39.,  64.,   4.,  74.,   0.,   0.,   0.,   0.],\n",
       "       [ 99.,  84.,  59.,  18.,  32.,  73.,   0.,   0.,   0.,   0.],\n",
       "       [  8.,  83.,   7.,  21.,  10.,  78.,   0.,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_inputs = np.array([np.concatenate((i,\n",
    "            [0 for j in range(10-len(i))])) for i in source_inputs])\n",
    "padded_inputs[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 13.,  73.,  66.,  15.,  58.,  16.,  34.,  14.,   0.,   0.],\n",
       "       [ 88.,  94.,  16.,  32.,  59.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [ 93.,  14.,  26.,  18.,  62.,  83.,   0.,   0.,   0.,   0.],\n",
       "       [  9.,  51.,  96.,  22.,  86.,  98.,   0.,   0.,   0.,   0.],\n",
       "       [ 37.,  46.,  80.,   2.,  84.,  14.,  30.,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_outputs = np.array([np.concatenate((i,\n",
    "            [0 for j in range(10-len(i))])) for i in target_inputs])\n",
    "padded_outputs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a mask to apply to the source and target\n",
    "\n",
    "The mask is used in the calculation of loss. The mask is 1 when the input is > 0 and 0 otherwise. When there are sentences which are shorter than the maximum length we pad them with 0s. So the RNN outputs that correspond to these 0s should not be included in the calculation of the loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  0.,  0.,  0.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  0.,  0.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_mask_array = 1.0*(padded_inputs>0)\n",
    "source_mask_array[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  0.,  0.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_mask_array = 1.0*(padded_outputs>0)\n",
    "target_mask_array[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-directional RNN to calculate the input matrix representation\n",
    "The first step is to run a bidirectional RNN and get all the states which are used to create the matrix representation of the input sentence. \n",
    "\n",
    "In the output matrix there will be one column per word which has two halves concatenated together:\n",
    "- a “forward representation”, i.e., a word and its left context (sentence read from left to right)\n",
    "- a “reverse representation”, i.e., a word and its right context (sentence read from right to left)\n",
    "\n",
    "We will use tensorflow's birectional_dynamic_RNN for this purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Parameters for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "source_vocab_size = 100\n",
    "target_vocab_size = 100\n",
    "embed_size = 20\n",
    "hidden_size = 25\n",
    "align_size = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define placeholder for the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "source_ids = tf.placeholder(dtype=tf.int32,shape=[None,None])\n",
    "source_seq_lens = tf.placeholder(dtype=tf.int32,shape=[None])\n",
    "source_mask = tf.placeholder(dtype=tf.float32,shape=[None,None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operation to look up embeddings for input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "source_embeddings = tf.get_variable('source_embedding_matrix',\n",
    "                            [source_vocab_size+1, embed_size])\n",
    "enc_inputs = tf.nn.embedding_lookup(source_embeddings, source_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Define RNN\n",
    "\n",
    "We will use GRU cells for both directions of the RNN and use tensorflow's bidirectional_dynamic_rnn to dynamically unroll the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enc_fw_cell = tf.contrib.rnn.GRUCell(hidden_size)\n",
    "enc_bw_cell = tf.contrib.rnn.GRUCell(hidden_size)\n",
    "\n",
    "enc_outputs, states = \\\n",
    "tf.nn.bidirectional_dynamic_rnn(cell_fw = enc_fw_cell, \n",
    "                                         cell_bw = enc_bw_cell,\n",
    "                                         inputs = enc_inputs,\n",
    "                                         sequence_length = source_seq_lens,\n",
    "                                         dtype = tf.float32)\n",
    "\n",
    "# concatenate the forward and backward representations\n",
    "concat_enc_outputs = tf.concat(enc_outputs, axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far this is not much different from running an ordinary RNN as you might do for a language modelling task. However now we need to implement a decoder with attention. Whilst tensorflow has functions that can simplify this process, let us go through it step-by-step and create all the necessary intermediary variables. \n",
    "\n",
    "First we need to calculate an initial state for the decoder. \n",
    "\n",
    "Dimensions of the different matrices.\n",
    "\n",
    "- X - batch_size x max_sequence_length\n",
    "- rnn_inputs - batch_size x max_sequence_length x source_embed_size\n",
    "- Each output - batch_size x max_sequence_length x encoder_state_size\n",
    "- F = concat_outputs - batch_size x (max_sequence_length\\*2) x encoder_state_size\n",
    "- U - encoder_state_size x decoder_state_size\n",
    "- bw_output1 - batch_size x encoder_state_size\n",
    "\n",
    "- s_0 = decoder_state = tf.matmul(U,tf.transpose(bw_output1))\n",
    "\n",
    "- s_0 = decoder_state - batch_size x decoder_state_size\n",
    "\n",
    "- W - [dim] x (max_sequence_length\\*2)\n",
    "\n",
    "- X - batch_size x [dim] x encoder_state_size\n",
    "\n",
    "- V - [dim] x \n",
    "\n",
    "\n",
    "- c_t - batch_size x target_embed_size\n",
    "- P - vocab_size x decoder_state_size\n",
    "- b - vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$U$ is called the attention energy. It is calculated by taking the dot product with every column in the source matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bw_enc_output1 = enc_outputs[-1][:,1,:] # this is the last hidden state of the encoder in the reverse direction\n",
    "U = tf.get_variable('U',[hidden_size,hidden_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$s_0$ this is the last hidden state of the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_state = tf.matmul(bw_enc_output1,U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$W$ is a learned parameter of the model - a weight matrix applied to the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 881,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = tf.placeholder(tf.int32)\n",
    "W = tf.get_variable('W',[1,2*hidden_size,align_size],dtype=tf.float32)\n",
    "W_rep = tf.tile(W,[batch_size,1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the calculation for $WF$ in advance as it does not depend on the output dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = tf.matmul(concat_enc_outputs,W_rep) # concat_enc_outputs is F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$V$ is a learned parameter of the model - weight matrix applied for transforming the previous hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "V = tf.get_variable('V',[hidden_size,align_size])\n",
    "v = tf.get_variable('v',[1,align_size,1])\n",
    "v_rep = tf.tile(v,[batch_size,1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P$ and $b$ are also learned parameters of the model - the weights and the biases for the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 886,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "P = tf.get_variable('P',[hidden_size,target_vocab_size+2])\n",
    "b = tf.get_variable('b',[target_vocab_size+2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An embedding matrix for the target vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 887,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_embeddings = tf.get_variable('target_embedding_matrix',\n",
    "                            [target_vocab_size+2, embed_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an RNN cell for the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 888,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('RNN'):\n",
    "    decoder_cell = tf.contrib.rnn.GRUCell(hidden_size)\n",
    "    \n",
    "decoder_embed = tf.nn.embedding_lookup(target_embeddings, \n",
    "                                       tf.ones([batch_size],tf.int32))\n",
    "\n",
    "decoder_output = tf.placeholder(tf.int32,[None,None])\n",
    "decoder_length = 10\n",
    "\n",
    "target_mask = tf.placeholder(dtype=tf.float32,shape=[None,None])\n",
    "target_seq_lens = tf.placeholder(dtype=tf.int32,shape=[None])\n",
    "\n",
    "total_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the decoder operations\n",
    "\n",
    "Here we will the operations of the decoder as outlined in slide 72 of Lecture 8. For each time step of the decoder, we calculate the attention weights and then apply this to the input to calculate the output at the current time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 895,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for t in range(decoder_length):\n",
    "    with tf.variable_scope('RNN'):\n",
    "        logits = []\n",
    "        if t > 0:\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "            \n",
    "        # compute attention\n",
    "        r_t = tf.matmul(decoder_state,V)\n",
    "        \n",
    "        tanh_input = X + tf.expand_dims(r_t,axis=1)\n",
    "        u_t = tf.matmul(tanh_input,v_rep)\n",
    "        u_t = tf.squeeze(u_t,axis=2)\n",
    "        \n",
    "        # for this time step calculate the attention weight for each word of the input\n",
    "        exp_u_t = tf.exp(u_t)\n",
    "        softmax_denom = tf.reduce_sum(exp_u_t*source_mask,axis=1,keep_dims=True)\n",
    "        a_t = exp_u_t/softmax_denom\n",
    "        a_t = a_t*source_mask\n",
    "        a_expn = tf.expand_dims(a_t,axis=1)\n",
    "        \n",
    "        # apply attention weights to the input matrix F\n",
    "        c_t = tf.matmul(a_expn,concat_enc_outputs)\n",
    "        c_t = tf.squeeze(c_t,axis=1)\n",
    "        \n",
    "        # use the attented input to calculate the \n",
    "        decoder_input = tf.concat([decoder_embed,c_t],\n",
    "                                  axis=1)\n",
    "        decoder_state,_ = decoder_cell(decoder_input,\n",
    "                                       decoder_state) \n",
    "        \n",
    "        logit = tf.nn.xw_plus_b(decoder_state,P,b)\n",
    "        \n",
    "        # choose the output word based on the argmax of the output logits \n",
    "        logits.append(tf.argmax(logit, 1))\n",
    "        \n",
    "        # transform the output ids into one hot vectors of length vocab size in order to calculate cross entropy\n",
    "        onehot = tf.one_hot(decoder_output[:,t],\n",
    "                            depth=target_vocab_size+2)\n",
    "        \n",
    "        # compute cross entropy loss \n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "                        logits = logit,\n",
    "                        labels = onehot\n",
    "                    )\n",
    "        \n",
    "        xe_mask = tf.cast(tf.greater(decoder_output[:,t],0),tf.float32)\n",
    "        \n",
    "        cross_entropy = xe_mask*cross_entropy\n",
    "        \n",
    "        loss = tf.reduce_sum(cross_entropy)\n",
    "\n",
    "        total_loss += loss\n",
    "    \n",
    "avg_loss = total_loss / tf.reduce_sum(target_mask)\n",
    "train_op = tf.train.AdamOptimizer(1e-4).minimize(avg_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the decoder \n",
    "\n",
    "Here we define a TensorFlow session, initialise the variables for the source and target, and execute the operations of the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set the parameters for the decoder\n",
    "num_examples = len(padded_inputs)\n",
    "batch_size_ = 6\n",
    "start = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(math.ceil(num_examples/batch_size_)):\n",
    "    start = i*batch_size_\n",
    "    end = start + batch_size_\n",
    "    with tf.Session() as sess:\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        sess.run(init_op)\n",
    "        logits_,avg_loss_,_ = sess.run([logits,avg_loss,train_op],\n",
    "                            feed_dict={\n",
    "                                       batch_size:min(num_examples, end) - start,\n",
    "                                       source_mask:source_mask_array[start:end,:],\n",
    "                                       target_mask:target_mask_array[start:end,:],\n",
    "                                       source_ids:padded_inputs[start:end], \n",
    "                                       source_seq_lens:source_lens[start:end],\n",
    "                                       target_seq_lens:target_lens[start:end],\n",
    "                                       decoder_output:padded_outputs[start:end,:],\n",
    "                                      })\n",
    "    print(logits_,avg_loss_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
